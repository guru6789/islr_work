---
title: "R Notebook"
output: 
  md_document:
    variant: markdown_github
---

Performing ridge regression and the lasso in order to predict Salary on the Hitters data.
```{r}
options(warn = -1)
library(ISLR)
Hitters = na.omit(Hitters) #remove missing values
```
We will use the `glmnet` package in order to perform ridge regression and the lasso. This function has slightly diﬀerent syntax from other model-ﬁtting functions. In particular, we must pass in an x matrix as well as a y vector.
```{r}
x = model.matrix(Salary ~ ., data = Hitters)[,-1]
y = Hitters$Salary
```


```{r}
library(glmnet)
grid = 10^seq(10, -2, length = 100)
ridge.mod = glmnet(x, y, alpha = 0, lambda = grid)  #alpha = 0 is ridge
```

Associated with each value of λ is a vector of ridge regression coeﬃcients, stored in a matrix that can be accessed by `coef()`
```{r}
dim(coef(ridge.mod))
```

In this case it's 20x100 matrix.

We expect the coeﬃcient estimates to be much smaller, in terms of l2 norm, when a large value of λ is used, as compared to when a small value of λ is used. These are the coeﬃcients when λ = 11,498, along with their l2 norm:

```{r}
ridge.mod$lambda[50]
coef(ridge.mod)[, 50]
sqrt(sum(coef(ridge.mod)[-1, 50]^2))
```

In contrast, here are the coeﬃcients when λ = 705, along with their l2 norm. Note the much larger l2 norm of the coeﬃcients associated with this smaller value of λ.
```{r}
ridge.mod$lambda[60]
coef(ridge.mod)[, 60]
sqrt(sum(coef(ridge.mod)[-1, 60]^2))
```
We can use the `predict()` function for a number of purposes. For instance, we can obtain the ridge regression coeﬃcients for a new value of λ, say 50
```{r}
predict(ridge.mod, s = 50, type = "coefficients")
```

We now split the samples into a training set and a test set in order to estimate the test error of ridge regression and the lasso. There are two common ways to randomly split a data set. The ﬁrst is to produce a random vector of TRUE, FALSE elements and select the observations corresponding to TRUE for the training data. The second is to randomly choose a subset of numbers between 1 and n; these can then be used as the indices for the training observations.Here we use latter.
```{r}
set.seed(1)
train = sample(1:nrow(x), nrow(x)/2)  #50%
test = -train
y.test = y[test]
```

Next we ﬁt a ridge regression model on the training set, and evaluate its MSE on the test set, using λ = 4.
```{r}
ridge.mod = glmnet(x[train,], y[train], alpha = 0, lambda = grid, thresh = 1e-12)
ridge.pred = predict(ridge.mod, s = 4, newx = x[test,])
mean((ridge.pred - y.test)^2)
```

