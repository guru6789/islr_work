---
title: "R Notebook"
output: 
  md_document:
    variant: markdown_github
---

Applying the best subset selection approach to the Hitters data. We wish to predict a baseball player’s Salary on the basis of various statistics associated with performance in the previous year.

```{r}
options(warn = -1)
library(ISLR)
names(Hitters)
dim(Hitters)
sum(is.na(Hitters$Salary))
```

Here `salary` is missing for 59 players. use `na.omit` to remove all the rows that have missing value.

```{r}
Hitters = na.omit(Hitters)
dim(Hitters)
sum(is.na(Hitters$Salary))
```

The `regsubsets()` function (part of the `leaps` library) performs best subset selection by identifying the best model that contains a given number of predictors, where best is quantiﬁed using RSS.

```{r}
library(leaps)
regfit.full = regsubsets(Salary ~ .,Hitters)
summary(regfit.full)
```

An asterisk indicates that given variable is included in model. For ex best two-variable model contains only `Hits` and `CRBI`

By default `regsubsets` only reports variable upto 8 variable model. use `nvmax` to return as many variables as desired.
```{r}
regfit.full = regsubsets(Salary ~ ., Hitters, nvmax = 19)
reg.summary = summary(regfit.full)
```

R2 statistic

```{r}
reg.summary$rsq
```

R2 statistic increases from 32% when only one variable included to almost 55% when all the variables are included.

Plotting RSS, adjusted R2, Cp, and BIC for all of the models at once will help us decide which model to select.
```{r}
par(mfrow = c(2,2))
plot(reg.summary$rss, xlab = "No of variables", ylab = "RSS", type="l")  #type="l"  to connect the plotted points with lines.

plot(reg.summary$adjr2, xlab = "No of variables", ylab = "Adjusted Rsq", type="l")
#which.max(reg.summary$adjr2) location of max point in vector => 11
points(11, reg.summary$adjr2[11], col = 'red', cex = 2, pch = 20)

plot(reg.summary$cp, xlab = "No of variables", ylab = "Cp", type = 'l')
#which.min(reg.summary$cp ) => 10
points(10, reg.summary$cp[10], col = "red", cex = 2, pch = 20)

plot(reg.summary$bic, xlab = "No of variabes", ylab = "BCC", type = 'l')
#which.min(reg.summary$bic ) => 6
points(6, reg.summary$bic[6], col = "red", cex = 2, pch = 20)
```
```{r}
plot(regfit.full ,scale="r2")
plot(regfit.full ,scale="adjr2")
plot(regfit.full ,scale="Cp")
plot(regfit.full ,scale="bic")
```

the modelwith lowest BIC contains 6 variables. to find coeficients,
```{r}
coef(regfit.full, 6)
```

****************************************************************************************************************************

Forward and Backward Stepwise Selection

****************************************************************************************************************************

```{r}

regfit.fwd = regsubsets(Salary ~ ., data = Hitters, nvmax = 19, method = "forward")
summary(regfit.fwd)

```
```{r}


regfit.bwd = regsubsets(Salary ~ ., data = Hitters, nvmax = 19, method = "backward")
summary(regfit.bwd)
```

we see that using forward stepwise selection, the best one variable model contains only `CRBI`, and the best two-variable model additionally includes `Hits`. For this data, the best one-variable through six variable models are each identical for best subset and forward selection. However, the best seven-variable models identiﬁed by forward stepwise selection, backward stepwise selection, and best subset selection are diﬀerent.
```{r}
coef(regfit.full, 7)
```
```{r}
coef(regfit.fwd, 7)
```
```{r}
coef(regfit.bwd, 7)
```

##################################################################################################################

choosing models using validation and cross-validation set approach.

##################################################################################################################

first divide dataset into trainig and testing
```{r}
set.seed(1)
train = sample(c(TRUE, FALSE), nrow(Hitters), rep = TRUE)
test = !train
```

then apply `regsubsets` using only training data.
```{r}
regfit.best = regsubsets(Salary ~ ., data = Hitters[train,], nvmax = 19)
coef(regfit.best, 10)
```

We now compute the validation set error for the best model of each model size. We ﬁrst make a model matrix from the test data.
```{r}
test.mat = model.matrix(Salary ~., data = Hitters[test,])
```
Now we run a loop, and for each size i, we extract the coeﬃcients from `regfit.best` for the best model of that size, multiply them into the appropriate columns of the test model matrix to form the predictions, and compute the test MSE.
```{r}
predict.regsubsets = function(object, newData, id, ...){
  form = as.formula(object$call[[2]])
  mat = model.matrix(form, newData)
  coefi = coef(object, id = id)
  xvars = names(coefi)
  mat[,xvars]%*%coefi
}
```

```{r}
regfit.best = regsubsets(Salary ~., data = Hitters, nvmax = 19)
coef(regfit.best, 10)
```

Using full dataset provide diffrent coefficients for 10 variable model than using just training dataset.We now try to choose among the models of diﬀerent sizes using crossvalidation. First, we create a vector that allocates each observation to one of k = 10 folds, and we create a matrix in which we will store the results.
```{r}
k = 10
set.seed(1)
folds = sample(1:k, nrow(Hitters), replace = T)
cv.errors = matrix(NA, k, 19, dimnames = list(NULL, paste(1:19)))
```
Now we write a for loop that performs cross-validation. In the jth fold, the elements of folds that equal j are in the test set, and the remainder are in the training set.
```{r}
for(j in 1:k){
  best.fit = regsubsets(Salary ~ ., data = Hitters[folds != j,], nvmax = 19)
  for(i in 1:19){
    pred = predict(best.fit, Hitters[folds == j,], id = i)
    cv.errors[j, i] = mean((Hitters$Salary[folds == j] - pred)^2)
  }
}
```
This has given us a 10×19 matrix, of which the (i,j)th element corresponds to the test MSE for the ith cross-validation fold for the best j-variable model.We use the `apply()` function to average over the columns of this matrix in order to obtain a vector for which the jth element is the crossvalidation error for the j-variable model.
```{r}
mean.cv.errors = apply(cv.errors, 2, mean)
mean.cv.errors
```
```{r}
par(mfrow = c(1,1))
plot(mean.cv.errors, type = 'b')
```
We see that cross-validation selects an 11-variable model. We now perform best subset selection on the full data set in order to obtain the 11-variable model.
```{r}
reg.best = regsubsets(Salary ~ .,data = Hitters, nvmax = 19)
coef(reg.best, 11)
```

